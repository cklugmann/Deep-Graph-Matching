{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "from os import listdir, makedirs\n",
    "from os.path import exists, isfile, isdir, join\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.spatial import Delaunay\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, please specify in which directory the VOC dataset can be found. Note that the `IMGS` and `ANNOTATIONS` folders must be subdirectories of `BASE_PATH`. The `GRAPH_PATH` folder is automatically created in the same folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = 'C:/VOC/VOCdevkit/VOC2011'\n",
    "IMGS = 'JPEGImages'\n",
    "ANNOTATIONS = 'annotations'\n",
    "GRAPH_PATH = 'graph'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subfolders(path):\n",
    "    return [name for name in listdir(path) if isdir(join(path, name))]\n",
    "\n",
    "def get_filenames(mypath):\n",
    "    return [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_xml(root):\n",
    "    img = root.find('image').text\n",
    "    category = root.find('category').text\n",
    "    bndbox = root.find('visible_bounds').attrib\n",
    "    for target in bndbox:\n",
    "        bndbox[target] = float(bndbox[target])\n",
    "    keypoints = root.find('keypoints').findall('keypoint')\n",
    "    keypoints = [keypoint.attrib for keypoint in keypoints]\n",
    "    for keypoint in keypoints:\n",
    "        x, y = float(keypoint['x']), float(keypoint['y'])\n",
    "        for attr_drop in ['visible', 'x', 'y', 'z']:\n",
    "            keypoint.pop(attr_drop, None)\n",
    "        keypoint['pos'] = (x, y)\n",
    "    return {'img': img, 'category': category, 'bndbox': bndbox, 'keypoints': keypoints}\n",
    "\n",
    "\n",
    "def get_data_from_file(path):\n",
    "    return get_data_from_xml(ET.parse(path).getroot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_per_class(path):\n",
    "    probs = {}\n",
    "    files = {}\n",
    "    for folder in get_subfolders(path):\n",
    "        files[folder] = get_filenames(join(path, folder))\n",
    "        probs[folder] = len(files[folder])\n",
    "    total_count = sum([val for _, val in probs.items()])\n",
    "    for key in probs.keys():\n",
    "        probs[key] /= total_count\n",
    "    return files, probs\n",
    "    \n",
    "\n",
    "def generate_data_from_dict(files_per_class, path):\n",
    "    total_data = {}\n",
    "    for folder in files_per_class.keys():\n",
    "        total_data[folder] = []\n",
    "        for file in files_per_class[folder]:\n",
    "            chosen_file = join(path, folder, file)\n",
    "            entry = get_data_from_file(chosen_file)\n",
    "            if len(entry['keypoints']) > 2:\n",
    "                total_data[folder].append(entry)\n",
    "    return total_data\n",
    "\n",
    "\n",
    "def generate_data(path):\n",
    "    files, probs = get_files_per_class(path)\n",
    "    return generate_data_from_dict(files, path), probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to know which classes exist in the dataset, you can execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'])\n"
     ]
    }
   ],
   "source": [
    "data_tmp, _ = generate_data(join(BASE_PATH, ANNOTATIONS))\n",
    "print(data_tmp.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code determines the maximum number of keypoints. You can skip this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of annotations: 20\n"
     ]
    }
   ],
   "source": [
    "curr_max = 0\n",
    "for class_name in data_tmp.keys():\n",
    "    class_data = data_tmp[class_name]\n",
    "    max_num_kps = max([len(entry['keypoints']) for entry in class_data])\n",
    "    if max_num_kps > curr_max:\n",
    "        curr_max = max_num_kps\n",
    "\n",
    "print('Maximum number of annotations:', curr_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delaunay_tri(keypoints):\n",
    "    points = [entry['pos'] for entry in keypoints]\n",
    "    points = np.array(points)\n",
    "    try:\n",
    "        tri = Delaunay(points)\n",
    "    except:\n",
    "        tri = None\n",
    "    return points, tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delaunay_to_adjacency(points, tri):\n",
    "    num_points, _ = points.shape\n",
    "    simplices = tri.simplices\n",
    "    _, cols = simplices.shape\n",
    "    simplices_flattened = simplices.reshape(-1,)\n",
    "    reps = np.repeat(simplices_flattened, repeats=cols)\n",
    "    tiles = np.tile(simplices, reps=cols).reshape(-1,)\n",
    "    res = np.zeros((num_points, num_points))\n",
    "    res[reps, tiles] = 1\n",
    "    np.fill_diagonal(res, 0)\n",
    "    return res\n",
    "\n",
    "\n",
    "def make_graph(keypoints):\n",
    "    points, tri = delaunay_tri(keypoints)\n",
    "    graph = None if tri is None else delaunay_to_adjacency(points, tri)\n",
    "    return points, tri, graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data_for_class(class_data, start_id=0, max_samples=None):\n",
    "    running_id = start_id\n",
    "    resulting_data = []\n",
    "    samples_per_class = len(class_data)\n",
    "    counter = 0\n",
    "    for id_source in range(samples_per_class - 1):\n",
    "        for id_target in range(id_source + 1, samples_per_class):\n",
    "            entry_s, entry_t = class_data[id_source], class_data[id_target]\n",
    "            kp_s = [kp['name'] for kp in entry_s['keypoints']]\n",
    "            kp_t = [kp['name'] for kp in entry_t['keypoints']]\n",
    "            kp_common = [kp for kp in kp_s if kp in kp_t]\n",
    "            if len(kp_common) > 2:\n",
    "                corresp_kp_s = [kp_dic for kp_dic in entry_s['keypoints'] if kp_dic['name'] in kp_common]\n",
    "                corresp_kp_t = [kp_dic for kp_dic in entry_t['keypoints'] if kp_dic['name'] in kp_common]\n",
    "                points_s, _, graph_s = make_graph(corresp_kp_s)\n",
    "                points_t, _, graph_t = make_graph(corresp_kp_t)\n",
    "                # graph_s == None or graph_t == None => Delaunay failed\n",
    "                if graph_s is not None and graph_t is not None:\n",
    "                    resulting_data.append({'id': running_id,\n",
    "                                           'img_s': entry_s['img'],\n",
    "                                           'img_t': entry_t['img'], \n",
    "                                           'cat': entry_s['category'],\n",
    "                                           'kps': kp_common,\n",
    "                                           'bndbox_s': [val for _, val in entry_s['bndbox'].items()],\n",
    "                                           'bndbox_t': [val for _, val in entry_t['bndbox'].items()],\n",
    "                                           'points_s': points_s,\n",
    "                                           'points_t': points_t,\n",
    "                                           'graph_s': graph_s,\n",
    "                                           'graph_t': graph_t\n",
    "                                          })\n",
    "                    running_id += 1\n",
    "                    counter += 1\n",
    "                    if max_samples is not None and counter >= max_samples:\n",
    "                        return resulting_data, running_id\n",
    "    return resulting_data, running_id\n",
    "\n",
    "\n",
    "def generate_data_according_to_prob(data, prob, max_samples=None):\n",
    "    running_id = 0\n",
    "    semantic_classes = list(data.keys())\n",
    "    if max_samples is not None:\n",
    "        max_samples = [int(p * max_samples) for _, p in prob.items()]\n",
    "    else:\n",
    "        max_samples = [None] * len(semantic_classes)\n",
    "    result_total = []\n",
    "    for idx, semantic_class in enumerate(semantic_classes):\n",
    "        res_class, rid = generate_training_data_for_class(data[semantic_class],\n",
    "                                                          start_id=running_id,\n",
    "                                                          max_samples=max_samples[idx])\n",
    "        running_id = rid\n",
    "        result_total.append(res_class)\n",
    "    return [item for sublist in result_total for item in sublist]\n",
    "\n",
    "\n",
    "def generate_training_data(annotation_path, max_samples=None):\n",
    "    data, prob = generate_data(annotation_path)\n",
    "    return generate_data_according_to_prob(data, prob, max_samples=max_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph_dirs(b_path, graph_dir):\n",
    "    graph_dir = join(b_path, graph_dir)\n",
    "    sub_dirs = ['adj', 'points']\n",
    "    for s_dir in sub_dirs:\n",
    "        sub_dir = join(graph_dir, s_dir)\n",
    "        if not exists(sub_dir):\n",
    "            makedirs(sub_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_to_files(data_total, graph_path):\n",
    "    random.shuffle(data_total)\n",
    "    n_total = len(data_total)\n",
    "    n_train = int(0.8 * n_total)\n",
    "    data = {}\n",
    "    data['train'] = data_total[:n_train]\n",
    "    data['val'] = data_total[n_train:]\n",
    "    out_files = {}\n",
    "    out_files['train'] = open(join(graph_path, 'train.txt'), 'a')\n",
    "    out_files['val'] = open(join(graph_path, 'val.txt'), 'a')\n",
    "    for mode in data.keys():\n",
    "        for entry in data[mode]:\n",
    "            row = [str(entry['id']),\n",
    "                   entry['img_s'],\n",
    "                   entry['img_t'],\n",
    "                   entry['cat'],\n",
    "                   *[str(val) for val in entry['bndbox_s']],\n",
    "                   *[str(val) for val in entry['bndbox_t']],\n",
    "                   *[kp for kp in entry['kps']]\n",
    "                  ]\n",
    "            out_files[mode].write('\\t'.join(row))\n",
    "            out_files[mode].write('\\n')\n",
    "            # Write serialized numpy objects\n",
    "            points_s, points_t = entry['points_s'], entry['points_t']\n",
    "            points = np.stack((points_s, points_t), axis=0)\n",
    "            np.save(join(graph_path, 'points', str(entry['id'])), points)\n",
    "            graph_s, graph_t = entry['graph_s'], entry['graph_t']\n",
    "            graph = np.stack((graph_s, graph_t), axis=0)\n",
    "            np.save(join(graph_path, 'adj', str(entry['id'])), graph)\n",
    "            \n",
    "    out_files['train'].close()\n",
    "    out_files['val'].close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the **crucial part** to build the pairs of source and target images and their graph topologies and store them accordingly. Change `max_samples` to specify how many image pairs should be sampled in total. It may take some time to generate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49990\n"
     ]
    }
   ],
   "source": [
    "mypath = join(BASE_PATH, ANNOTATIONS)\n",
    "res_data = generate_training_data(mypath, max_samples=50000)\n",
    "print(len(res_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the required directories if they do not already exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_graph_dirs(BASE_PATH, GRAPH_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And write the generated data into the respective files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data_to_files(res_data, join(BASE_PATH, GRAPH_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
